from __future__ import annotations
import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Optional
from dateutil.relativedelta import relativedelta
import statsmodels.api as sm
from pathlib import Path

# =========================================================
# 1. Load monthly returns (and synthetic generator if needed)
# =========================================================

def load_monthly_returns(root_dir: Path) -> dict[str, pd.Series]:
    out = {}
    for path in sorted(root_dir.glob("*_Monthly_Revenues.csv")):
        ticker = path.stem.replace("_Monthly_Revenues", "")
        df = pd.read_csv(path)
        df['date']   = pd.to_datetime(df[['year','month']].assign(day=1))
        df['return'] = pd.to_numeric(df['return'], errors='coerce')
        out[ticker]  = df.set_index('date')['return'].sort_index()
    return out

def generate_synthetic_log_returns(n_years: int = 10,
                                   seed: int = 123) -> pd.Series:
    """
    Synthetic monthly log-return series with:
      - sinusoidal seasonal pattern (~±1%)
      - uniform noise in [-0.02, 0.02]
    Noise is generated by a hand-made LCG so that Python and R
    can produce identical series for the same seed.
    """
    n_months = 12 * n_years
    idx = pd.date_range("2000-01-01", periods=n_months, freq="MS")

    # seasonal pattern (~±1%)
    month = idx.month
    seasonal_pattern = 0.01 * np.sin(2 * np.pi * (month - 1) / 12.0)

    # custom LCG: x_{n+1} = (a*x_n) mod m, with m=2^31-1, a=16807
    m = 2**31 - 1
    a = 16807

    x = seed % m
    if x == 0:
        x = 1

    u = np.empty(n_months, dtype=float)
    for i in range(n_months):
        x = (a * x) % m
        u[i] = x / m

    # uniform noise in [-0.02, 0.02]
    noise = 0.04 * (u - 0.5)

    log_ret = seasonal_pattern + noise
    return pd.Series(log_ret, index=idx, name="log_return")


# =========================================================
# 2. EWMA volatility (RiskMetrics-style) – not used in table
# =========================================================

def ewma_sigma_next(log_returns: pd.Series,
                    lam: float = 0.97) -> float:
    """
    One-step-ahead EWMA volatility forecast sigma_{T+1}.

    Recursion:
        sigma_{t+1}^2 = lam * sigma_t^2 + (1 - lam) * r_t^2

    Initialise sigma_1^2 with the sample variance.
    """
    r = log_returns.dropna().to_numpy(dtype=float)
    if r.size == 0:
        return np.nan

    if r.size > 1:
        sigma2 = float(np.var(r, ddof=1))
    else:
        sigma2 = float(r[0] ** 2)

    for rt in r:
        sigma2 = lam * sigma2 + (1.0 - lam) * float(rt) ** 2

    return float(np.sqrt(max(sigma2, 0.0)))


# =========================================================
# 3. DVR: Dummy-Variable Regression (beta + p-value)
# =========================================================

@dataclass
class DVRResult:
    forecast: float      # alpha_hat + beta_hat (mean in target month)
    alpha_hat: float     # mean outside target month
    beta_hat: float      # difference: r_m - r_not_m (seasonal effect)
    se_beta: float       # standard error of dummy coefficient
    t_beta: float        # t-stat for dummy
    p_beta: float        # p-value for dummy
    significant_10: bool # True if p_beta < 0.10


def dvr_forecast(log_returns: pd.Series,
                 forecast_month: Optional[pd.Timestamp] = None,
                 alpha_level: float = 0.10) -> DVRResult:
    """
    Dummy-variable regression for a single calendar month m.

    R counterpart:
      r_t ~ alpha + beta * D_t  via lm(r ~ D)
    """
    log_returns = log_returns.dropna()
    if log_returns.empty:
        return DVRResult(*(np.nan,) * 6, False)

    if forecast_month is None:
        last = log_returns.index[-1]
        forecast_month = last + relativedelta(months=1)
    target_month = forecast_month.month

    df = pd.DataFrame({"r": log_returns})
    df["D"] = (df.index.month == target_month).astype(float)

    X = sm.add_constant(df["D"])
    ols_res = sm.OLS(df["r"], X).fit()

    alpha_hat = float(ols_res.params["const"])
    beta_hat = float(ols_res.params["D"])
    se_beta  = float(ols_res.bse["D"])
    t_beta   = float(ols_res.tvalues["D"])
    p_beta   = float(ols_res.pvalues["D"])

    forecast = alpha_hat + beta_hat
    significant = (p_beta < alpha_level)

    return DVRResult(forecast, alpha_hat, beta_hat, se_beta, t_beta, p_beta, significant)


# =========================================================
# 4. SSA helpers
# =========================================================

def _build_trajectory_matrix(x: np.ndarray, L: int) -> np.ndarray:
    """
    Build Hankel trajectory matrix X (L x K) from series x of length N.
    """
    N = x.size
    K = N - L + 1
    X = np.empty((L, K), dtype=float)
    for i in range(K):
        X[:, i] = x[i:i + L]
    return X


# =========================================================
# 5. Basic SSA forecast (recurrent SSA)
# =========================================================

def basic_ssa_forecast(log_returns: pd.Series,
                       L: int = 36,
                       r: int = 12) -> float:
    """
    Basic recurrent SSA one-step-ahead forecast.
    """
    x = log_returns.dropna().to_numpy(dtype=float)
    N = x.size
    if N < 2:
        return np.nan

    if L >= N:
        L = N - 1
    if L < 2:
        L = 2

    K = N - L + 1
    X = _build_trajectory_matrix(x, L)  # L x K

    # SVD of X
    U, d, Vt = np.linalg.svd(X, full_matrices=False)
    r_eff = min(r, U.shape[1])

    # Reconstruct signal matrix from first r_eff eigentriples
    X_signal = np.zeros_like(X)
    for i in range(r_eff):
        ui = U[:, i:i+1]       # L x 1
        vi = Vt[i:i+1, :].T    # K x 1
        X_signal += d[i] * (ui @ vi.T)

    # Diagonal averaging (Hankelization) -> recon[0..N-1]
    recon = np.zeros(N, dtype=float)
    counts = np.zeros(N, dtype=int)
    for i in range(L):
        for j in range(K):
            idx = i + j
            recon[idx] += X_signal[i, j]
            counts[idx] += 1
    counts[counts == 0] = 1
    recon /= counts

    # SSA recurrence coefficients (using left singular vectors U)
    if L <= 1:
        return float(recon[-1])

    nu2 = 0.0
    R_vec = np.zeros(L - 1, dtype=float)   # [a_{L-1}, ..., a_1]
    for i in range(r_eff):
        P_i = U[:, i]
        pi_i = P_i[L - 1]                  # last coordinate
        nu2 += pi_i**2
        R_vec += pi_i * P_i[:L - 1]        # first L-1 coordinates

    if abs(1.0 - nu2) < 1e-8:
        # Degenerate: fallback to AR(1) on recon
        if N < 3:
            return float(recon[-1])
        y1 = recon[1:]
        ylag = recon[:-1]
        denom = float(np.dot(ylag, ylag))
        if denom == 0.0:
            return float(recon[-1])
        phi = float(np.dot(ylag, y1) / denom)
        return float(recon[-1] * phi)

    R_vec /= (1.0 - nu2)   # [a_{L-1},...,a_1]
    a = R_vec[::-1]        # [a_1,...,a_{L-1}]

    # Tail of reconstructed series needed for recurrence
    if N > L:
        x_tail = recon[N - L + 1: N]   # length L-1
    else:
        x_tail = recon.copy()
        if x_tail.size < (L - 1):
            pad = np.full((L - 1 - x_tail.size,), x_tail[0], dtype=float)
            x_tail = np.concatenate([pad, x_tail])

    x_tail_rev = x_tail[::-1]          # x_N, x_{N-1}, ..., x_{N-L+2}
    forecast = float(np.dot(a, x_tail_rev))  # \hat x_{N+1}
    return forecast


# =========================================================
# 6. RLSSA: Robust L1 Singular Spectrum Analysis (Hawkins AL1)
# =========================================================

def _weighted_median(values: np.ndarray, weights: np.ndarray) -> float:
    """
    Weighted median mirroring matrixStats::weightedMedian(x, w,
      na.rm = TRUE, interpolate = FALSE, ties = \"weighted\").
    """
    values = np.asarray(values, dtype=float)
    weights = np.asarray(weights, dtype=float)

    # Drop NA in x or w (na.rm = TRUE)
    mask = ~np.isnan(values) & ~np.isnan(weights)
    if not np.any(mask):
        return 0.0

    values = values[mask]
    weights = weights[mask]

    if values.size == 0:
        return 0.0

    # Negative weights treated as zero
    weights = np.where(weights > 0.0, weights, 0.0)

    total_w = float(np.sum(weights))
    if total_w == 0.0:
        return 0.0

    # Special case: any infinite weights => treat all Inf weights equal, others 0
    if np.any(np.isinf(weights)):
        inf_mask = np.isinf(weights)
        vals_inf = values[inf_mask]
        if vals_inf.size == 0:
            return 0.0
        return float(np.median(vals_inf))

    # Sort by values
    order = np.argsort(values)
    v_sorted = values[order]
    w_sorted = weights[order]

    # Collapse to unique x's with summed weights
    uniq_vals, inv = np.unique(v_sorted, return_inverse=True)
    uniq_w = np.zeros_like(uniq_vals, dtype=float)
    for idx, g in enumerate(inv):
        uniq_w[g] += w_sorted[idx]

    cum_w = np.cumsum(uniq_w)
    S = float(cum_w[-1])
    half = 0.5 * S

    w_less = np.concatenate(([0.0], cum_w[:-1]))
    w_greater = S - cum_w

    cand_mask = (w_less <= half) & (w_greater <= half)
    cand_idx = np.where(cand_mask)[0]

    if cand_idx.size == 0:
        k = int(np.searchsorted(cum_w, half, side="left"))
        return float(uniq_vals[k])

    if cand_idx.size == 1:
        return float(uniq_vals[cand_idx[0]])

    # Two candidates: ties = "weighted"
    i_low, i_high = int(cand_idx[0]), int(cand_idx[1])
    w_le_low = cum_w[i_low]
    if i_high > 0:
        w_ge_high = S - cum_w[i_high - 1]
    else:
        w_ge_high = S

    num = w_le_low * uniq_vals[i_low] + w_ge_high * uniq_vals[i_high]
    den = w_le_low + w_ge_high
    if den == 0.0:
        return float(0.5 * (uniq_vals[i_low] + uniq_vals[i_high]))
    return float(num / den)


def _l1_reg_coef(x: np.ndarray, a: np.ndarray, eps: float) -> float:
    """
    L1RegCoef from R robustSvd: one-dimensional L1 regression coefficient.
    """
    x = np.asarray(x, dtype=float)
    a = np.asarray(a, dtype=float)

    keep = (np.abs(a) > eps) & (~np.isnan(x))
    if not np.any(keep):
        return 0.0

    x_keep = x[keep]
    a_keep = a[keep]
    z = x_keep / a_keep
    w = np.abs(a_keep)

    return _weighted_median(z, w)


def _l1_eigen(x: np.ndarray, a: np.ndarray, b: np.ndarray, eps: float) -> float:
    """
    L1Eigen from R robustSvd.
    """
    x_vec = np.asarray(x, dtype=float).reshape(-1)
    ab = np.outer(a, b).reshape(-1)

    keep = (np.abs(ab) > eps) & (~np.isnan(x_vec))
    if not np.any(keep):
        return 0.0

    xk = x_vec[keep]
    abk = ab[keep]
    z = xk / abk
    w = np.abs(abk)

    return _weighted_median(z, w)


def robust_svd(X: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Python port of the R robustSvd() function (Hawkins AL1 algorithm).

    Returns (u, d, v) such that X ≈ u diag(d) v^T.
    """
    X = np.asarray(X, dtype=float)
    m, n = X.shape
    eps = np.finfo(float).eps

    svdu = np.empty((m, n), dtype=float)
    svdv = np.empty((n, n), dtype=float)
    svdd = np.empty(n, dtype=float)

    X_resid = X.copy()

    for k in range(n):
        # initial ak: median of abs(x) along rows
        ak = np.nanmedian(np.abs(X_resid), axis=1)
        converged = False
        while not converged:
            akprev = ak.copy()

            # c: apply L1RegCoef over columns
            c = np.empty(n, dtype=float)
            for j in range(n):
                c[j] = _l1_reg_coef(X_resid[:, j], ak, eps)

            norm_c = float(np.sqrt(np.sum(c**2)))
            if norm_c == 0.0:
                bk = np.zeros(n, dtype=float)
            else:
                bk = c / norm_c

            # d: apply L1RegCoef over rows
            d = np.empty(m, dtype=float)
            for i in range(m):
                d[i] = _l1_reg_coef(X_resid[i, :], bk, eps)

            norm_d = float(np.sqrt(np.sum(d**2)))
            if norm_d == 0.0:
                ak = np.zeros(m, dtype=float)
            else:
                ak = d / norm_d

            diff = ak - akprev
            if float(np.sum(diff**2)) < 1e-10:
                converged = True

        eigenk = _l1_eigen(X_resid, ak, bk, eps)

        # Deflate the residual X
        X_resid = X_resid - eigenk * np.outer(ak, bk)

        # Store eigen triple
        svdu[:, k] = ak
        svdv[:, k] = bk
        svdd[k] = eigenk

    return svdu, svdd, svdv


def rlssa_forecast(log_returns: pd.Series,
                   L: int = 36,
                   r: int = 12) -> float:
    """
    One-step-ahead RLSSA forecast.

    (i) Robust L1 SVD -> robust low-rank signal matrix S
    (ii) Hankelization -> recon
    (iii) Classical SSA LRR coefficients on S + recurrent forecast
    """
    x = log_returns.dropna().to_numpy(dtype=float)
    N = x.size
    if N < 2:
        return np.nan

    if L >= N:
        L = N - 1
    if L < 2:
        L = 2

    K = N - L + 1
    X = _build_trajectory_matrix(x, L)  # L x K

    # Stage (i): robust SVD
    try:
        U_r, d_r, V_r = robust_svd(X)
    except Exception:
        # fallback: classical SVD
        U_r, d_r, Vt_r = np.linalg.svd(X, full_matrices=False)
        V_r = Vt_r.T

    r_eff = min(r, U_r.shape[1])

    # Robust low-rank signal matrix S
    S = np.zeros_like(X)
    for i in range(r_eff):
        ui = U_r[:, i:i+1]   # L x 1
        vi = V_r[:, i:i+1]   # K x 1
        S += d_r[i] * (ui @ vi.T)

    # Hankelization -> recon (robust fitted series)
    recon = np.zeros(N, dtype=float)
    counts = np.zeros(N, dtype=int)
    for i in range(L):
        for j in range(K):
            idx = i + j
            recon[idx] += S[i, j]
            counts[idx] += 1
    counts[counts == 0] = 1
    recon /= counts

    # Stage (ii): classical SVD on S to get LRR coefficients
    U2, d2, Vt2 = np.linalg.svd(S, full_matrices=False)
    r_eff2 = min(r, U2.shape[1])

    if L <= 1:
        return float(recon[-1])

    nu2 = 0.0
    R_vec = np.zeros(L - 1, dtype=float)   # [a_{L-1},...,a_1]
    for j in range(r_eff2):
        P_j = U2[:L-1, j]
        phi_j = U2[L-1, j]
        nu2 += phi_j**2
        R_vec += phi_j * P_j

    if abs(1.0 - nu2) < 1e-8:
        # fallback AR(1) on recon
        if N < 3:
            return float(recon[-1])
        y1 = recon[1:]
        ylag = recon[:-1]
        denom = float(np.dot(ylag, ylag))
        if denom == 0.0:
            return float(recon[-1])
        phi = float(np.dot(ylag, y1) / denom)
        return float(recon[-1] * phi)

    R_vec /= (1.0 - nu2)
    a = R_vec[::-1]   # [a_1,...,a_{L-1}]

    # Stage (iii): 1-step recurrent forecast
    if N > L:
        x_tail = recon[N - L + 1: N]   # length L-1
    else:
        x_tail = recon.copy()
        if x_tail.size < (L - 1):
            pad = np.full((L - 1 - x_tail.size,), x_tail[0], dtype=float)
            x_tail = np.concatenate([pad, x_tail])
    x_tail_rev = x_tail[::-1]
    forecast = float(np.dot(a, x_tail_rev))
    return forecast


# =========================================================
# 7. Scaling experiment: beta for DVR, forecasts for SSA / RLSSA
# =========================================================

def run_scaling_experiment(c: float = 2.0) -> pd.DataFrame:
    """
    Scaling experiment for CC (2005–2014).

    - For DVR: use beta_hat (seasonal effect) and its p-value.
    - For SSA and RLSSA: use the one-step-ahead forecast.
    - Compare values for c = 1 vs c = c.
    """
    base = Path().resolve().parent.parent / "Complete Data"
    returns = load_monthly_returns(base / "All_Monthly_Log_Return_Data")
    cc_simple = returns["CC"]  # this is the CC return Series

    # Restrict to 2005-01 to 2014-12 and convert to log returns
    cc_2005_2014 = cc_simple.loc['2005-01-01':'2014-12-01']
    #log_ret = np.log1p(cc_2005_2014)
    log_ret = cc_2005_2014

    # Scaled series: c * log_ret  (e.g. c = 2 in the thesis example)
    log_ret_scaled = log_ret*2

    N = len(log_ret)
    last_month = log_ret.index[-1].month
    target_month = 1 if last_month == 12 else last_month + 1
    forecast_month = log_ret.index[-1] + relativedelta(months=1)

    # DVR (beta + p-value)
    dvr_base   = dvr_forecast(log_ret,        forecast_month=forecast_month)
    dvr_scaled = dvr_forecast(log_ret_scaled, forecast_month=forecast_month)

    # SSA forecasts
    f_ssa_base   = basic_ssa_forecast(log_ret,        L=36, r=12)
    f_ssa_scaled = basic_ssa_forecast(log_ret_scaled, L=36, r=12)

    # RLSSA forecasts
    f_rls_base   = rlssa_forecast(log_ret,        L=36, r=12)
    f_rls_scaled = rlssa_forecast(log_ret_scaled, L=36, r=12)

    def safe_ratio(num, den):
        if den is None or np.isnan(den) or den == 0.0:
            return np.nan
        return num / den

    rows = []

    # DVR row: beta is the "statistic"
    rows.append({
        "Model": "DVR",
        "Statistic": "beta",
        "Value c=1": dvr_base.beta_hat,
        f"Value c={c:g}": dvr_scaled.beta_hat,
    })

    # SSA row: forecast is the "statistic"
    rows.append({
        "Model": "SSA",
        "Statistic": "forecast",
        "Value c=1": f_ssa_base,
        f"Value c={c:g}": f_ssa_scaled,
    })

    # RLSSA row: forecast is the "statistic"
    rows.append({
        "Model": "RLSSA",
        "Statistic": "forecast",
        "Value c=1": f_rls_base,
        f"Value c={c:g}": f_rls_scaled,
    })

    result = pd.DataFrame(rows)

    # Pretty printing
    pd.set_option("display.float_format", lambda x: f"{x: .6g}")

    print(f"\n=== Scaling experiment (c = {c}) ===\n")
    print(f"Series length: {N} months")
    print(f"Target calendar month (1=Jan,...,12=Dec): {target_month}\n")

    print("DVR beta (seasonal effect) and SSA/RLSSA forecasts under scaling:\n")
    print(result.to_string(index=False))

    return result


# =========================================================
# 8. Run when executed as script
# =========================================================

if __name__ == "__main__":
    run_scaling_experiment(c=2.0)
